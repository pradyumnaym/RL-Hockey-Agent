batch_size: 256
grad_steps: 16

# number of episodes to collect experience from before starting training
start_training_after: 100

eval_freq: 1200
log_freq: 300
save_freq: 5000
eval_episodes: 100

device: 'cuda'
max_steps_in_episode: 250
max_episodes: 100000

agent_name: TD3

trainer:
    _target_: common.trainer.Trainer
    env:
        _target_: sac.custom_env.SinglePlayerHockeyEnv
        weak_mode: True
        #reward_scheme: '2'
    logger:
        _target_: common.logger.Logger
    replay_buffer:
        _target_: common.replay_buffer.PrioritizedReplayBuffer
        max_episodes: ${max_episodes}
        alpha: 0.6
        beta: 0.4
        max_size: 1000000
        device: ${device}
    action_noise:
        _target_: td3.noise.PinkNoise
        sigma: 0.2
        seq_len: ${max_steps_in_episode}
        action_dim: 4
        max_episodes: ${max_episodes}
        n_annealing_steps: 20                   # number of steps to anneal the noise to 0 by 80% of the training

agent:
    actor_lr: 0.0003
    critic_lr: 0.0003

    actor_lr_milestones: [20000, 40000, 70000]
    critic_lr_milestones: [20000, 40000, 70000]

    actor_lr_gamma: 0.5
    critic_lr_gamma: 0.5

    discount_factor: 0.95

    critic_target_update_freq: 1
    critic_target_tau: 0.005

    policy_noise: 0.23
    noise_clip: 0.5

    policy_delay: 2
    tau: 0.003
    
    actor:
        hidden_dims: [512, 512, 512]
        activation: leaky_relu
        output_activation: tanh

    critic:
        hidden_dims: [512, 512, 512]
        activation: leaky_relu

    resume_from: null

# opponent pooler
opponent_pooler:
    weak_prob:      [0.6, 0.3, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
    strong_prob:    [0.4, 0.5, 0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
    self_prob:      [0.0, 0.1, 0.1, 0.2, 0.3, 0.2, 0.2, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5]
    custom_prob:    [0.0, 0.1, 0.4, 0.6, 0.5, 0.6, 0.6, 0.4, 0.4, 0.3, 0.3, 0.3, 0.3]
    custom_weight_paths: ['checkpoints/model_best_sac_14.pth',                      # best sac agent        
                          'checkpoints/model_best_sac_2102_23h55.pth',              # best sac agent (large model)
                          'outputs/2025-02-21/23-13-30/model_best.pth',             # best strong agent
                          # 'outputs/2025-02-20/22-41-32/model_best.pth',             # best weak agent
                          ]
    self_weights_dir: outputs/2025-02-22/22-29-11
    max_episodes: ${max_episodes}
    update_self_opponent_freq: 1000
    collect_self_after: 30000