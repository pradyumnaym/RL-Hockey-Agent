batch_size: 128
grad_steps: 16

# number of episodes to collect experience from before starting training
start_training_after: 100

eval_freq: 1200
log_freq: 300
eval_episodes: 100

device: 'cuda'
max_steps_in_episode: 250
max_episodes: 60000

agent_name: TD3

trainer:
    _target_: common.trainer.Trainer
    env:
        _target_: sac.custom_env.SinglePlayerHockeyEnv
        weak_mode: True
        #reward_scheme: '2'
    logger:
        _target_: common.logger.Logger
    replay_buffer:
        _target_: common.replay_buffer.PrioritizedReplayBuffer
        max_episodes: ${max_episodes}
        alpha: 0.6
        beta: 0.4
        max_size: 100000
        device: ${device}
    action_noise:
        _target_: td3.noise.PinkNoise
        sigma: 0.4
        seq_len: ${max_steps_in_episode}
        action_dim: 4
        max_episodes: ${max_episodes}
        n_annealing_steps: 20                   # number of steps to anneal the noise to 0 by 80% of the training

agent:
    actor_lr: 0.0003
    critic_lr: 0.0003

    actor_lr_milestones: [10000, 20000, 40000]
    critic_lr_milestones: [10000, 20000, 40000]

    actor_lr_gamma: 0.5
    critic_lr_gamma: 0.5

    discount_factor: 0.95

    critic_target_update_freq: 1
    critic_target_tau: 0.005

    policy_noise: 0.23
    noise_clip: 0.5

    policy_delay: 2
    tau: 0.003
    
    actor:
        hidden_dims: [256, 256]
        activation: leaky_relu
        output_activation: tanh

    critic:
        hidden_dims: [256, 256]
        activation: leaky_relu

    resume_from: /mnt/lustre/work/ponsmoll/pba549/RL-Hockey-Agent/outputs/2025-02-20/22-41-32/model_best.pth

# opponent pooler - values can either be floats or list of floats. 
# If list, the max_episodes should also be mentioned.
opponent_pooler:
    weak_prob:   0.0              # last value is not used
    strong_prob: 1.0
    self_prob:   0.0
    max_episodes: ${max_episodes}
    update_self_opponent_freq: 1000
