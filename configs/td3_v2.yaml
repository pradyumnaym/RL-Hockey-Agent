batch_size: 256
grad_steps: 16

# number of episodes to collect experience from before starting training
start_training_after: 100

eval_freq: 500
log_freq: 300
save_freq: 10000
eval_episodes: 100
custom_opponents_eval_count: 3    # only the first 3 opponents from the custom opponents list will be used for evaluation

device: 'cuda'
max_steps_in_episode: 250
max_episodes: 20000

agent_name: TD3

trainer:
    _target_: common.trainer.Trainer
    env:
        _target_: sac.custom_env.SinglePlayerHockeyEnv
        weak_mode: True
        #reward_scheme: '2'
    logger:
        _target_: common.logger.Logger
    # replay_buffer:
    #     _target_: common.replay_buffer.ReplayBuffer
    #     max_size: 100000
    replay_buffer:
        _target_: common.replay_buffer.PrioritizedReplayBuffer
        max_episodes: ${max_episodes}
        alpha: 0.6
        beta: 0.4
        max_size: 1000000
        device: ${device}
    action_noise:
        _target_: td3.noise.PinkNoise
        sigma: 0.35
        seq_len: ${max_steps_in_episode}
        action_dim: 4
        max_episodes: ${max_episodes}
        n_annealing_steps: 20                   # number of steps to anneal the noise to 0 by 80% of the training

agent:
    actor_lr: 0.0003
    critic_lr: 0.0003

    actor_lr_milestones: [10000, 30000, 40000]
    critic_lr_milestones: [10000, 30000, 40000]

    actor_lr_gamma: 0.5
    critic_lr_gamma: 0.5

    discount_factor: 0.95

    critic_target_update_freq: 1
    critic_target_tau: 0.005

    policy_noise: 0.23
    noise_clip: 0.5

    policy_delay: 2
    tau: 0.003
    
    actor:
        hidden_dims: [512, 512, 512]
        activation: leaky_relu
        output_activation: tanh

    critic:
        hidden_dims: [512, 512, 512]
        activation: leaky_relu

    resume_from: null

# opponent pooler
opponent_pooler:
    weak_prob:   [0.9, 0.2]
    strong_prob: [0.1, 0.8]
    self_prob:   [0.0, 0.0]
    custom_prob: [0.0, 0.0]
    max_episodes: ${max_episodes}
    custom_weight_paths: []
    update_self_opponent_freq: 1000
    collect_self_after: 5000
    
# opponent pooler
# opponent_pooler:
#     weak_prob:   [0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
#     strong_prob: [0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
#     self_prob:   [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.5, 0.5, 0.5, 0.6, 0.6]
#     custom_prob: [0.3, 0.3, 0.3, 0.5, 0.5, 0.5, 0.5, 0.5, 0.3, 0.3, 0.3, 0.2, 0.2]
#     custom_weight_paths: [
#         'outputs/2025-02-23/14-04-14/model_best_35000.pth',
#         'outputs/2025-02-24/13-55-37/model_best_60000.pth',
#         'outputs/2025-02-23/21-53-03/model_last.pth',
#         'checkpoints/model_best_Feb23_1808_SAC.pth',              # best sac agent 23rd (large model)       
#         'checkpoints/model_best_sac_2102_23h55.pth',              # best sac agent (large model)
#         'outputs/2025-02-21/23-13-30/model_best.pth',             # best strong agent
#         'outputs/2025-02-23/11-04-56/model_best_25000.pth',       # best strong agent (large model)
        
#         'outputs/2025-02-23/14-04-14/model_best.pth',
#         'outputs/2025-02-23/14-04-14/model_last.pth',
#         'checkpoints/model_best_sac_14.pth',                      # best sac agent 22nd

#         'outputs_sac/2025-02-22/23-55-09/model_best.pth',
#         'outputs_sac/2025-02-22/23-55-09/model_best.pth',
#         'outputs_sac/2025-02-22/09-55-08/model_best.pth',
#         'outputs_sac/2025-02-21/23-09-13/model_best.pth',
#         'outputs_sac/2025-02-21/09-53-26/model_best.pth',
#         'outputs_sac/2025-02-20/08-28-24/model_best.pth', 
#         'outputs_sac/2025-02-20/09-25-09/model_best.pth', 
#         'outputs_sac/2025-02-20/20-12-39/model_best.pth',
#         'outputs_sac/2025-02-21/13-01-54/model_best.pth'
#     ]
#     self_weights_dir: outputs/2025-02-22/22-29-11               # best agent trained against sac (small)       
#     max_episodes: ${max_episodes}
#     update_self_opponent_freq: 1000
#     collect_self_after: 5000