batch_size: 128
grad_steps: 32

# number of episodes to collect experience from before starting training
start_training_after: 100

eval_freq: 500
log_freq: 100
eval_episodes: 1000

device: 'cpu'
max_steps_in_episode: 250
max_episodes: 20000

agent_name: SAC

trainer:
    _target_: common.trainer.Trainer
    env:
        _target_: sac.custom_env.SinglePlayerHockeyEnv
        weak_mode: True
    logger:
        _target_: common.logger.Logger
    replay_buffer:
        _target_: common.replay_buffer.ReplayBuffer
        max_size: 100000

agent:
    actor_hidden_dims: [256, 256]
    critic_hidden_dims: [256, 256]

    actor_lr: 0.0001
    critic_lr: 0.0001
    alpha_lr: 0.0001

    actor_lr_milestones: [5000, 10000]
    critic_lr_milestones: [5000, 10000]

    actor_lr_gamma: 0.5
    critic_lr_gamma: 0.5

    discount_factor: 0.95

    critic_target_update_freq: 1
    critic_target_tau: 0.005

    entropy_tuning: true
    alpha: 0.2
    alpha_lr_milestones: [10000]
    alpha_lr_gamma: 0.5

    resume_from: null

# opponent pooler
opponent_pooler:
    weak_prob: 1.0
    strong_prob: 0.0
    self_prob: 0.0
    update_self_opponent_freq: 1000
